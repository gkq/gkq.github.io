---
    author: gkq
    comments: false
    date: 2016-11-06
    layout: post
    title: 机器学习之范数规则
    categories: Machine Learning
    tags: 正则化, L1范数, L2范数

---

#前言#
机器学习中，模型训练一般要加上正则项，它的目的是什么？不同的正则项的又有什么不同呢？下面这两篇博客给出了较为详细的解释，这里做下简单总结。

[机器学习中的范数规则化之（一）L0、L1与L2范数](http://blog.csdn.net/zouxy09/article/details/24971995/)

[机器学习中的范数规则化之（二）核范数与规则项参数选择](http://blog.csdn.net/zouxy09/article/details/24972869)

## 过拟合与正则化 ##

监督学习问题的目标是规则化参数的同时最小化误差。最小化误差是为了使模型拟合训练数据，而规则化参数是防止过分拟合。前者是经验风险最小，后者是结构风险最小。为什么要加入正则化参数？

- 增强模型泛化能力：因为参数过多，会导致我们的模型复杂度上升，容易过拟合，也即虽然训练误差会减少，但预测新样本的能力会变差，即泛化能力差。
奥卡姆剃刀原理
- 约束模型：可以将人对这个模型的先验知识融入到模型的学习中，使得模型具有期望的特性，例如稀疏、低秩、平滑等。从贝叶斯估计的角度来看，规则化对应于模型的先验知识。

一般来说，监督学习可以看做最小化下面的目标函数：

![](http://img.blog.csdn.net/20140504122253546?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvem91eHkwOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

> 对于第一项Loss函数，如果是Square loss，那就是最小二乘了；如果是Hinge Loss，那就是著名的SVM了；如果是exp-Loss，那就是牛逼的 Boosting了；如果是log-Loss，那就是Logistic Regression了；还有等等。不同的loss函数，具有不同的拟合特性，这个也得就具体问题具体分析的。但这里，我们先不究loss函数的问题，我们把目光转向“规则项Ω(w)”。

规则化函数Ω(w)也有很多种选择，一般是模型复杂度的单调递增函数，模型越复杂，规则化值就越大。比如，规则化项可以是模型参数向量的范数。然而，不同的选择对参数w的约束不同，取得的效果也不同，但我们在论文中常见的都聚集在：零范数、一范数、二范数、迹范数、Frobenius范数和核范数等等。这么多范数，到底它们表达啥意思？具有啥能力？什么时候才能用？什么时候需要用呢？不急不急，下面我们挑几个常见的娓娓道来。

## L0范数和L1范数 ##
L0是NP难问题，很难优化求解，L1是L0的最优凸近似，而且比L0范数更容易求解。
为什么要实现稀疏？

- 特征选择：使得模型更简单
- 可解释性：模型简单，越容易解释

## L2范数 ##
“岭回归”（Ridge Regression），它的强大功效是改善机器学习里面一个非常重要的问题：过拟合。
它和L1不同，不是使大部分参数为0，而是接近0，而越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象（why?限制了参数很小，实际上就限制了多项式某些分量的影响很小，这样就相当于减少参数个数。)

总之，通过L2范数，我们可以实现了对模型空间的限制，从而在一定程度上避免了过拟合。
L2的好处？

- 学习理论的角度来说，L2范数可以防止过拟合，提升模型的泛化能力。
- 优化的角度：改善条件数（矩阵范数相关），条件数过大一方面会造成解不可靠，另一方面拖慢收敛速度。正则项的引入使得原问题变成λ-strongly convex（λ强凸），加快收敛。在梯度下降中，目标函数收敛速率的上界实际上是和矩阵XTX的 condition number有关，XTX的 condition number 越小，上界就越小，也就是收敛速度会越快。

优化两大难题：

- 局部最小，即局部最小过多，容易陷入
- ill-condition病态问题

L1和L2的区别：

- 下降速度：L1较快
- 模型空间的限制：L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。Lasso在特征选择时候非常有用，而Ridge就只是一种规则化而已。